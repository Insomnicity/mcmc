{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo and Animation using Matplotlib\n",
    "\n",
    "Let us say we make n (say 15) observations of an event (employee height), which is mapped to a number (Random Variable). Our belief (The prior) that the average value of follows a normal distribution that has a mean of 70 and standard deviation is 6. We have no knowledge of the distribution that generated the data (The posterior)\n",
    "\n",
    "The requirement is, say, to generate 1000 samples from the unknown posterior distribution.\n",
    "\n",
    "Markov Chain Monte Carlo enables sample generation from the posterior distribution, without knowing the parameters of the posterior distribution.\n",
    "\n",
    "MCMC constructs series of 'steps', where each step represents an update to the belies about the distribution the observed data is from. The steps are constructed such that the belief doesn't 'converge' to a single value. However, if you look at the chain of steps themselves, you may be able to observe that a (large enough) sequence of k steps is similar to another k step sequence in its characteristiics. Each of these k steps then represents a sample from the unknown posterior distribution.\n",
    "\n",
    "There are three steps to generating 1 sample and and entry/input condition:\n",
    "Input condition: you must have a current belief or guess about the unknown (posterior) distribution.\n",
    "\n",
    "1. Update the current guess to a 'proposed' guess using a proposal mechanism. \n",
    "The proposal mechanism is a way to perturb the current guess to a different value, using a proposal distribution. The proposal is then tested using the acceptance mechanism.\n",
    "\n",
    "Below, we sample mean from a normal distribution and sigma from an inverse gamma distribution. A later version of the notebook will talk more about the sigma.\n",
    "\n",
    "The choice of the proposal distribution has some implication on how easy is it to generate samples. Remember, by perturbing the current guess, we have only generated a proposal for the parameter, we have not declared it to an official sample yet. If our proposal mechanism generates guesses that always move 'far' from the current guess, the acceptance rate may be low. On the other hand,guesses very near to the current guess may mean that the MCMC may take 'too long' to converge.\n",
    "\n",
    "2. Determine the likelihood of observing the data with current guess and with proposal\n",
    "When a proposal is generated in step 1, we need to check if it explains the observed data (i.e. our 15 height observations) any better than the current guess. How do we measure the 'better' or 'worse' amd how do we quantify it? \n",
    "\n",
    "We need to use a 'likelihood' function, or at least a version that may be proportional to the likelihood function. What is likelihood? I will take a couple of lines to explain, more knowledgeable practistioners can choose to skip.\n",
    "\n",
    "Likelihood function is just what it says it is. Likelihood(data) is the probability that \"exactly this data\" will be observed, given a probability distribution. \n",
    "\n",
    "This is how you calculate likelihood function:\n",
    "a. Calculate probability of observing each portion of data separately\n",
    "b. Assuming independence betwen all the observations, multiply all the probabilitoes together. That is it.\n",
    "\n",
    "Note: Ideally, log likelihoods should be used to reduce computational inaccuracies as likelihoods could be very small numbers.\n",
    "\n",
    "\n",
    "Recall that our current guess and the proposal guess represent a parameter for the probability function used in the above calculation. Therefore the likelihood will be different as the guessed parameter changes.\n",
    "\n",
    "3. Proposal Acceptance/Rejection\n",
    "If the likelihood of the data (being observed) increases with tne new guess, we accept the guess.\n",
    "\n",
    "If the likelihood of the data (being observed) decreases with tne new guess, we sometimes accept the guess and sometimes accept the guess. A good way to think is, we want to accept worse outcomes with lower probability than less worse outcomes.\n",
    "\n",
    "so our acceptance function looks like \n",
    "    p_accept = min(1, (likelihood(data) with proposal)/(likelihood(data) with current guess))\n",
    "\n",
    "If the proposal is accepted, the proposal is added as a sample from mcmc; the proposal now becomes the current guess and the you can go back to step 1 to generate the next sample.\n",
    "\n",
    "#### Why does this work?\n",
    "Why do this series of steps work to generate a sample from the true posterior distribution? After all, we don't see a convergence to the 'actual' distribution. The proposals just seem to be bouncing around! also, how can the proposal be a sample itself?\n",
    "\n",
    "The answer is, with each mcmc step, the acceptance algorithm (Called the Metropolis-Hastings sampler) tries to push towards a distribution that explains the data better, but balances that against the need for exploring the sample space. Since we don't know the true posterior distribution, only accepting proposals that explain the data better wcould move the proposal towards a local convergence, which may lead to biased samples. The exploratory part of the sampler aceepts proposals that worsen the likelihood, but with a lower probability. Here, you can see that the acceptance function is shaping the sample space based on the explanatory power of the sample. One can see that proposal means with lower explanatory power are further from a 'true' mean and have a lower chance of being selected as a sample. This fits in well with the idea that if draws were being done from the true posterior, samples that explain the data better have a higher chance of being drawn and vice-versa.\n",
    "\n",
    "#### Does my initial guess matter?\n",
    "Ideally, no. Practically, maybe. If the initial guess is far away from the 'true' value of the parameter, the mcmc may take a longer time to 'converge'. However, it is expected to converge after a while. Since we don't know what convergence will exactly look like, it may be practial to throw away first few (10?, 1000?, 10000?) and take the ones after that\n",
    "\n",
    "\n",
    "#### Does it matter how far the proposal is from the current guess?\n",
    "The width of the jump is a tuning question, balancing between quicker converge versus the acceptance probability for a proposals. Ability to make a bigger jump may at times lead to faster convergence, but may lower the ratio of acceptances to proposals made.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of animated plots using matplotlib for mcmc model based on bayesian updates.\n",
    "\n",
    "This is created using python 3.6\n",
    "Step 1: import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from scipy.stats import norm, invgamma\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to determine if the code is running in a notebook or on a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ipython_info():\n",
    "    ip = False\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        ip = 'notebook'\n",
    "    elif 'IPython' in sys.modules:\n",
    "        ip = 'terminal'\n",
    "    return ip=='notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Provide a prior. This is your guess on the mu and sigma\n",
    "\n",
    "Step 2: You need to move the mu (and sigma) using the proposal distributions for mu and sigma\n",
    "\n",
    "\n",
    "Step 3: Calculate the likelihood of the data with the prior estimates of x\n",
    "\n",
    "Step 4: Calculate the likelihood of the data with the new estimates of the mu and sigma, estimated by moving from the previous step.\n",
    "\n",
    "Step 5: Calculate Probability of the data * probability of the prior, using the prior mu and sigma (P(x/theta) * P(theta)\n",
    "\n",
    "Step 6: Calculate Probability of data * probability of the mu_updated, using the new mu and sigma (P(x/thetanew) * P(thetanew)\n",
    "\n",
    "Step 7: Create acceptance where a better move is always acceptable, a bad move is sometimes acceptable, with a bad move more acceptable than a worse move\n",
    "\n",
    "\n",
    "For Later\n",
    "\n",
    "Plug in an alternative to Metropolis algorithm and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a bayesian update function\n",
    "    This step creates a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_mu_posterior_given_sigma(data, mu_0, sigma_0):\n",
    "    n = len(data)\n",
    "    sigma = invgamma.rvs(a=4.07, loc=0, scale=1, size=1, random_state=None)\n",
    "    mu_post = (mu_0/sigma_0**2 + data.sum()/sigma**2)/(1.0/sigma_0**2 + n/sigma**2)\n",
    "    sigma_post = 1.0/(1.0/ sigma_0**2 + n/sigma**2)\n",
    "    return norm(mu_post, np.sqrt(sigma_post))\n",
    "\n",
    "def ln_likelihood(mu_in,sigma_in,input_data,i):\n",
    "    return np.sum(np.log(norm(mu_in,sigma_in).pdf(input_data)))\n",
    "\n",
    "def likelihood(mu_in,sigma_in,input_data,i):\n",
    "    return np.exp(ln_likelihood(mu_in,sigma_in,input_data,i))\n",
    "\n",
    "def accept(mu_current,sigma_current,mu_proposal,sigma_proposal,data,i):\n",
    "    # P(data given mu_current and sigma_current)*P(mu_current given initial prior)\n",
    "    data_likely_mu_current=likelihood(mu_current, sigma_current,data,i)\n",
    "    mu_current_likely_prior=likelihood(mu_prior_init, sd_prior_init,mu_current,i)\n",
    "    # P(data given mu_proposal and sigma_proposal)*P(mu_proposal given initial prior)\n",
    "    data_likely_mu_proposal = likelihood(mu_proposal, sigma_proposal,data,i)\n",
    "    mu_proposal_likely_prior = likelihood(mu_prior_init, sd_prior_init,mu_proposal,i)\n",
    "    # acceptance/rejection algorithm\n",
    "    accept_sum= (data_likely_mu_proposal/data_likely_mu_current) * (mu_proposal_likely_prior / mu_current_likely_prior)\n",
    "    if (norm(0,1).rvs() <accept_sum):\n",
    "        accepted= True\n",
    "        mu_current = mu_proposal\n",
    "        sigma_current = sigma_proposal\n",
    "    else:\n",
    "        accepted =False\n",
    "\n",
    "    \n",
    "def update_dist(i):\n",
    "    # We need to create global variables as the variables will be accesses and updated from more than one function\n",
    "    #We want to view the path that our updated guesses follow. These are stored in the posterior\n",
    "    posterior.append(mu_current)\n",
    "    # This is the mechanism to create a new guess. We take the current value of the mean and use it in a normal distribution\n",
    "    # to generate a new proposed value.\n",
    "    # The move_control acts as \n",
    "    mu_proposal = norm(mu_current, move_control).rvs()\n",
    "    # sigma_proposal = norm(sigma_current, 0.2*sigma_current).rvs()\n",
    "    sigma_proposal = invgamma.rvs(a=4.07, loc=0, scale=1, size=1, random_state=None)\n",
    "    accept(mu_current,sigma_current,mu_proposal,sigma_proposal,data,i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 100)\n",
    "data = np.random.randn(len(x))\n",
    "move_control=.4\n",
    "#The prior\n",
    "mu_prior_init=0.5\n",
    "sd_prior_init=1.2\n",
    "prior=norm(mu_prior_init, sd_prior_init).pdf(x)\n",
    "# Initiql guess is same as prior, cold start to the algo\n",
    "mu_current = mu_prior_init\n",
    "sigma_current = sd_prior_init\n",
    "mu_proposal = mu_current\n",
    "sigma_proposal = sigma_current\n",
    "# mu_prior_mu=0\n",
    "# mu_prior_sd=1\n",
    "posterior=[mu_current]\n",
    "accepted_posterior=[mu_current]\n",
    "accepted=True\n",
    "color = 'g' if accepted else 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_likely_mu_current=likelihood(mu_current, sigma_current,data,1)\n",
    "mu_current_likely_prior=likelihood(mu_prior_init, sd_prior_init,mu_current,1)\n",
    "# P(data given mu_proposal and sigma_proposal)*P(mu_proposal given initial prior)\n",
    "data_likely_mu_proposal = likelihood(mu_proposal, sigma_proposal,data,1)\n",
    "mu_proposal_likely_prior = likelihood(mu_prior_init, sd_prior_init,mu_proposal,1)\n",
    "posterior_analytical = normal_mu_posterior_given_sigma(data, mu_prior_init, sd_prior_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(3,2, figsize=(8,12))\n",
    "# fig.figsize=(12,12)\n",
    "fig.suptitle('Iteration %i' % (1))\n",
    "\n",
    "#First figure - plots the prior distribution and shows change to proposed mean. The curve is fixed and teh vertical lines change\n",
    "ax1.plot(x,prior) # The prior curve is plotted\n",
    "ax1.set_xlim(-3, 3)\n",
    "ax1.set_ylim(0, 0.42)\n",
    "\n",
    "c1_curr_mu, = ax1.plot([], [], lw=2, linestyle='--',color='b') # vertical line represents current mean\n",
    "# proposed \\mu ,vertical line represents proposed mean, color will be green if accepted, else red\n",
    "c1_prop_mu, = ax1.plot([], [], lw=2,marker='o',linestyle=':', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second figure. Changing distribution as the proposed mean is accepted and \n",
    "ax2.set_xlim(-3, 3)\n",
    "ax2.set_ylim(0, 1.0)\n",
    "sns.distplot(data, kde=False, norm_hist=True, ax=ax2)\n",
    "c2, = ax2.plot([], [], lw=2)\n",
    "c2_curr_mu, = ax2.plot([], [],color='b', linestyle='--', label='mu_current')\n",
    "c2_prop_mu, = ax2.plot([],[], color=color, linestyle='--', label='mu_proposal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Figure - Show the analytical closed form posterior and how the mcmc samples look like\n",
    "ax3.set_xlim(-3, 3)\n",
    "ax3.set_ylim(0, 2.3)\n",
    "ax3.plot(x,posterior_analytical.pdf(x))\n",
    "\n",
    "# Fourth Figure - Plotting the path of mu_current\n",
    "iterlength=300\n",
    "ax4.set_xlim(0, iterlength)\n",
    "ax4.set_ylim(-3,3)\n",
    "c4, = ax4.plot([],[], lw=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Figure 5 creates histogram of accepted mu_currrent values against the analytically calculated posterior with a fixed sigma\n",
    "ax5.set_xlim(-3, 3)\n",
    "ax5.set_ylim(0, 2.3)\n",
    "ax5.plot(x,posterior_analytical.pdf(x))\n",
    "ax6.set_xlim(0, iterlength)\n",
    "ax6.set_ylim(-3,3)\n",
    "c6, = ax6.plot([],[], lw=2)\n",
    "plt.subplots_adjust(top=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    posterior.append(mu_current)\n",
    "    accepted_posterior.append(mu_current)\n",
    "    return c1_curr_mu, c1_prop_mu,c2,c2_curr_mu, c2_prop_mu,c4,c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_proposal(i):\n",
    "    global x, data, accepted, mu_current_likely_prior, mu_proposal_likely_prior,iterlength\n",
    "    global posterior, accepted_posterior\n",
    "    global mu_current, mu_proposal, sigma_current, sigma_proposal\n",
    "    global data_likely_mu_current, data_likely_mu_proposal\n",
    "    # Plot prior\n",
    "    # update_dist(i)\n",
    "    ########################\n",
    "#     posterior.append(mu_current)\n",
    "    # This is the mechanism to create a new guess. We take the current value of the mean and use it in a normal distribution\n",
    "    # to generate a new proposed value.\n",
    "    # The move_control controls how far the proposal jumps. \n",
    "    mu_proposal = norm(mu_current, move_control).rvs()\n",
    "    # For sigma_proposal, note that the proposed jump is not taken from an inverse gamma distribution\n",
    "    sigma_proposal = norm(sigma_current, 0.2*sigma_current).rvs()\n",
    "    #accept(mu_current,sigma_current,mu_proposal,sigma_proposal,data,i)\n",
    "    ############################################################\n",
    "        # P(data given mu_current and sigma_current)*P(mu_current given initial prior)\n",
    "    data_likely_mu_current=likelihood(mu_current, sigma_current,data,i)\n",
    "    mu_current_likely_prior=likelihood(mu_prior_init, sd_prior_init,mu_current,i)\n",
    "    # P(data given mu_proposal and sigma_proposal)*P(mu_proposal given initial prior)\n",
    "    data_likely_mu_proposal = likelihood(mu_proposal, sigma_proposal,data,i)\n",
    "    mu_proposal_likely_prior = likelihood(mu_prior_init, sd_prior_init,mu_proposal,i)\n",
    "    # acceptance/rejection algorithm\n",
    "    accept_sum= (data_likely_mu_proposal/data_likely_mu_current) * (mu_proposal_likely_prior / mu_current_likely_prior)\n",
    "    if (norm(0,1).rvs() <accept_sum):\n",
    "        accepted= True\n",
    "        mu_current = mu_proposal\n",
    "        sigma_current = sigma_proposal\n",
    "    else:\n",
    "        accepted =False\n",
    "    ###############################################\n",
    "    fig.suptitle('Iteration %i' % (i))\n",
    "    c1_curr_mu.set_data([mu_current] * 2, [0, mu_current_likely_prior]) \n",
    "    \n",
    "    # line showing probability of the current guess\n",
    "    c1_prop_mu.set_data([mu_proposal] * 2, [0, mu_proposal_likely_prior])\n",
    "    ax1.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2))\n",
    "    ax1.set(ylabel='Probability Density', title='mu_current=%.2f \\nmu_current_likely_prior= %.2f\\nmu_proposal=%.2f \\nmu_proposal_likely_prior = %.2f' % \n",
    "            (mu_current, mu_current_likely_prior, mu_proposal, mu_proposal_likely_prior))\n",
    "# set up the second curve\n",
    "    y = norm(loc=mu_proposal, scale=sigma_proposal).pdf(x)\n",
    "    c2.set_data(x, y)\n",
    "\n",
    "    c2_curr_mu.set_data([mu_current] * 2, [0, 1.0])\n",
    "    c2_prop_mu.set_data([mu_proposal] * 2, [0, 1.0])\n",
    "\n",
    "#     ax2.annotate(\"\", xy=(mu_proposal, 0.2), xytext=(mu_current, 0.2),\n",
    "#                  arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    ax2.set(title='Likelihood of mu_proposal|mu_current \\nmu_current=%.2f\\nlikelihood = %.2f\\nmu_proposal=%.2f \\nlikelihood = %.2f' % \n",
    "            (mu_current, data_likely_mu_current, mu_proposal, data_likely_mu_proposal))\n",
    "    \n",
    "\n",
    "    if accepted:\n",
    "        # Update position\n",
    "        mu_current = mu_proposal\n",
    "        sigma_current = sigma_proposal\n",
    "        accepted_posterior.append(mu_current)\n",
    "    color = 'g' if accepted else 'r'    \n",
    "    posterior.append(mu_current) # We will append even if the proposal is not accepted\n",
    "    \n",
    "    \n",
    "    ax3.clear() # clearing the histogram before redrawing it\n",
    "    ax3.set_xlim(-3, 3)\n",
    "    ax3.set_ylim(0, 2.3)\n",
    "    ax3.plot(x,posterior_analytical.pdf(x))\n",
    "    ax3.set(title = 'Histogram: \\mu proposal \\n against the bayesian calculated posterior')\n",
    "    sns.distplot(posterior, kde=False, norm_hist=True, ax=ax3)\n",
    "    # Set curve 4 as the plot of the path of mu_current with every iteration\n",
    "    ax4.set(title = 'Path of \\mu proposal')\n",
    "    c4.set_data(np.arange(len(posterior)),posterior)\n",
    " ##################\n",
    "    ax5.clear() # clearing the histogram before redrawing it)\n",
    "    ax5.set_xlim(-3, 3)\n",
    "    ax5.set_ylim(0, 2.3)\n",
    "    ax5.plot(x,posterior_analytical.pdf(x))\n",
    "    ax5.set(title='Histogram: \\mu current \\n against the bayesian calculated posterior')\n",
    "    sns.distplot(accepted_posterior, kde=False, norm_hist=True, ax=ax5)\n",
    "    # Set curve 4 as the plot of the path of mu_current with every iteration\n",
    "    ax6.set(title='Path of \\mu current')\n",
    "    c6.set_data(np.arange(len(accepted_posterior)),accepted_posterior)\n",
    "    plt.subplots_adjust(top=0.8, left = 0.1)\n",
    "    return c1_curr_mu, c1_prop_mu,c2,c2_curr_mu, c2_prop_mu,c4,c6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "anim = FuncAnimation(fig, plot_proposal, init_func=init, \n",
    "                               frames=iterlength, interval=200, blit=True)\n",
    "HTML(anim.to_html5_video()) #if ipython_info() else plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
